import os
from supabase import create_client
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

load_dotenv()

def seed():
    # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_KEY")
    
    if not url or not key:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –∑–∞–¥–∞–Ω—ã SUPABASE_URL –∏–ª–∏ KEY –≤ .env")
        return

    print("–ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Supabase...")
    supabase = create_client(url, key)
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (—Å–∫–∞—á–∞–µ—Ç—Å—è 1 —Ä–∞–∑)
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # 3. –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    file_path = "data/knowledge.txt"
    if not os.path.exists(file_path):
        print(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    if not text.strip():
        print("‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–æ—á–∫–∏ (—á–∞–Ω–∫–æ–≤) –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º
    chunks = [c.strip() for c in text.split("\n\n") if c.strip()]
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞.")

    # 4. –ó–∞–ª–∏–≤–∫–∞
    for i, chunk in enumerate(chunks):
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ü–∏—Ñ—Ä—ã
        vector = model.encode(chunk).tolist()
        
        data = {
            "content": chunk,
            "embedding": vector,
            "metadata": {"source": "knowledge.txt"}
        }
        
        try:
            supabase.table("knowledge_base").insert(data).execute()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç {i+1}/{len(chunks)}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ {i+1}: {e}")

    print("üéâ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

if __name__ == "__main__":
    seed()
